{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment for testing\n",
    "def initialize_reward(states,goal):\n",
    "    reward = {}\n",
    "    for s in states:\n",
    "        if s == goal:\n",
    "            reward[s] = 1\n",
    "        else:\n",
    "            reward[s] = -1\n",
    "    return reward\n",
    "\n",
    "\n",
    "def testing_environment():\n",
    "    actionspace = {'up' : (0,1), 'down' : (0,-1), 'left' : (-1,0), 'right' : (1,0),'none' :(0,0)}\n",
    "\n",
    "   \n",
    "\n",
    "    def transition (state,action):\n",
    "        next_state = tuple(np.asarray(state) + np.asarray(actionspace[action]))\n",
    "        if (state,next_state) in barriers or (next_state,state) in barriers:\n",
    "            return state\n",
    "        return next_state\n",
    "\n",
    "    def actions(new_state):\n",
    "        actions = {}\n",
    "        if new_state[0] > 1:\n",
    "            actions['left'] = (-1,0)\n",
    "        if new_state[0]  < maxcol:\n",
    "            actions['right'] = (1,0)\n",
    "        if new_state[1] > 1:\n",
    "            actions['down'] = (0,-1)\n",
    "        if new_state[1]  < maxrow:\n",
    "            actions['up'] = (0,1)\n",
    "        actions['none'] = (0,0)\n",
    "        return actions\n",
    "\n",
    "    def transition_prob(s_next,s,a):\n",
    "        return s_next == s\n",
    "    return transition, actions, transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value Iteration for the policies\n",
    "def Compute_Value_Function(states,actions,reward):\n",
    "    value_func = {}\n",
    "    for s in states:\n",
    "        value_func[s] = 0\n",
    "    error = 1\n",
    "    while error > 0:\n",
    "        error = 0\n",
    "        for s in states:\n",
    "            value = value_func[s]\n",
    "            value_func[s] = Return_Maximum_Action(s,value_func,actions(s),reward)[0]\n",
    "            error = max(error, abs(value - value_func[s]))\n",
    "    return value_func\n",
    "\n",
    "\n",
    "def Return_Maximum_Action(s,value_func,actions,reward):\n",
    "    action  = 0\n",
    "    new_value = -1000\n",
    "    for a in actions.keys():    \n",
    "        expected =  (gamma*value_func[transition(s,a)]+reward[s]) + (1-p)*(sum([(value_func[transition(s,b)]+reward[transition(s,b)]) for b in actions.keys() if b != a]))\n",
    "        if expected > new_value:\n",
    "            new_value = expected\n",
    "            action = a\n",
    "\n",
    "    \n",
    "    return (new_value, action)\n",
    "\n",
    "def optimal_action(s,value_func,actions,beta,reward):\n",
    "    out = []\n",
    "    total = 0\n",
    "    for a in actions.keys():    \n",
    "        q =  (gamma*p*value_func[transition(s,a)]+reward[s]) + (1-p)*(sum([(value_func[transition(s,b)]+reward[transition(s,b)]) for b in actions.keys() if b != a]))\n",
    "        out.append((a,np.exp(beta*q)))\n",
    "        total += np.exp(beta*q)\n",
    "    return [(x,y/total) for x,y in out]\n",
    "        \n",
    "def Compute_Policy(value_func,reward):\n",
    "    policy_dict = {}\n",
    "    for s in states:\n",
    "        policy_dict[s] = optimal_action(s,value_func,actions(s),b,reward)\n",
    "    return policy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_inference(sequence, goals, goal_prob, policy,transition_prob):\n",
    "    goal_probabilities = []\n",
    "    for g in goals:\n",
    "        p_seq = 1\n",
    "        for s in sequence[1:]:\n",
    "            p_snext = 0\n",
    "            for a,action_prob in policy[g][s]:\n",
    "                p_snext += transition_prob(transition(s,a),s,a)*action_prob\n",
    "            p_seq *= p_snext\n",
    "        goal_probabilities.append((g,p_seq))\n",
    "    return goal_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[((2, 10), 0.0007866250271018762),\n",
       "  ((4, 4), 0.00012772720748122307),\n",
       "  ((5, 5), 0.00018539107232044965),\n",
       "  ((6, 6), 0.00036932825832728804)]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Main Cell: Executes the model\n",
    "goals = [(2,10),(4,4),(5,5),(6,6)]\n",
    "gamma = 0.9\n",
    "p = 1\n",
    "b = 1\n",
    "maxrow = 19\n",
    "maxcol = 7\n",
    "barriers = []\n",
    "sequence = [(1,2),(1,3),(2,3),(3,3),(4,3)]\n",
    "output = []\n",
    "states = [(x,y) for x in range(1,maxcol+1) for y in range(1,maxrow+1)]\n",
    "transition, actions, transition_prob = testing_environment()\n",
    "rewards = {}\n",
    "value = {}\n",
    "policy = {}\n",
    "for g in goals:\n",
    "    rewards[g] = initialize_reward(states,g)\n",
    "    value[g] = Compute_Value_Function(states,actions,rewards[g])\n",
    "    policy[g] = Compute_Policy(value[g],rewards[g])\n",
    "output.append(goal_inference(sequence, goals, 1/len(goals), policy,transition_prob))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
